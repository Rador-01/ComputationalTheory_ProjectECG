\documentclass[sigconf]{acmart}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\setcopyright{none}
\acmConference[Conference'17]{ACM Conference}{July 2017}{Washington, DC, USA}
\acmYear{2025}
\acmISBN{978-x-xxxx-xxxx-x/YYYY/MM}
\acmDOI{10.1145/nnnnnnn.nnnnnnn}

\begin{document}

\title{Scanpath Pattern Recognition for ECG Interpretation Using Hidden Markov Models}

\maketitle

\author{Riad Benbrahim}
\email{riad.benbrahim@um6p.ma}
\affiliation{%
  \institution{Mohammed VI Polytechnic University}
  \city{Rabat}
  \country{Morocco}
}

\author{Mohamed Amine El Bacha}
\email{mohamedamine.elbacha@um6p.ma}
\affiliation{%
  \institution{Mohammed VI Polytechnic University}
  \city{Rabat}
  \country{Morocco}
}

\author{Youssef Kaya}
\email{youssef.kaya@um6p.ma}
\affiliation{%
  \institution{Mohammed VI Polytechnic University}
  \city{Rabat}
  \country{Morocco}
}


\begin{abstract}
The visual behavior of individuals—particularly their scanpaths when inspecting images—encodes essential information about their cognitive state, interests, attitudes, and even underlying health conditions. Analyzing these patterns has significant implications across multiple domains, especially in clinical settings where scanpath dynamics during ECG interpretation can reveal expertise-related differences. Leveraging computational models to characterize these gaze patterns enables the extraction of additional subject-specific features from ECG readings and supports the automatic discrimination between expert and novice clinicians.

In clinical practice, a scanpath  is represented as an ordered sequence of gaze fixations across distinct ECG regions. This sequence is converted into a symbolic string suitable for computational analysis. Under a Hidden Markov Model (HMM) framework, the hidden states correspond to the clinician's underlying cognitive processes, whereas the observable states represent the ECG segments being fixated. By learning the probabilistic relationships between these components, the model infers the most likely sequence of cognitive steps traversed during the interpretation process based on the recorded scanpath.

The evaluation relies on scanpath datasets obtained during ECG interpretation, using either real open-source data—such as the MIT ECG Eye-Tracking Dataset—or synthesized datasets collected from clinicians and medical students. In both cases, scanpaths are labeled according to expertise level, enabling the model to distinguish professional reading strategies from novice patterns.

The proposed model successfully distinguished expert scanpaths from novice patterns with a clear separation in likelihood scores, indicating strong discriminative capability. The probabilistic transitions learned from expert data revealed consistent and clinically meaningful viewing strategies, while synthesized or real datasets produced comparable performance trends. These results demonstrate that the chosen computational model can effectively capture systematic ECG-reading behavior and provide interpretable insight into the underlying cognitive process.
\end{abstract}

\keywords{Hidden Markov Models, Scanpath Analysis, Eye-tracking, ECG Interpretation, Medical Expertise, Pattern Recognition, Computational Theory}



\section{Introduction}

The interpretation of 12-lead electrocardiograms (ECGs) is a critical clinical task that demands systematic visual analysis and robust pattern-recognition skills. Expert cardiologists employ well-structured scanning strategies that enable comprehensive examination of all relevant cardiac features, whereas novice readers often exhibit inefficient or inconsistent viewing patterns that may result in incomplete assessments. Quantifying and characterizing these differences carries substantial importance for medical education, expertise evaluation, and the development of diagnostic support tools.

Eye-tracking technology offers high-precision measurements of visual attention by capturing scanpaths—the ordered sequences of fixations and saccadic movements made during ECG interpretation. Although prior studies have analyzed scanpath data using statistical approaches and machine-learning techniques, these methods frequently lack theoretical grounding and interpretability. In this work, We propose a Hidden Markov Model (HMM) framework that explicitly models the relationship between observable gaze positions on ECG leads and hidden cognitive diagnostic phases. This two-layer representation captures a key insight: what we observe through eye-tracking (Lead II, V1, aVF, etc.) does not directly reveal cognitive intent. A fixation on Lead II might indicate rhythm analysis, P-wave examination, or general scanning—the mapping is probabilistic and context-dependent.e of visual exploration patterns.

\subsection{Paper Organization}

The remainder of this paper is structured as follows. Section 2 reviews prior work on scanpath analysis and computational models applied to visual interpretation tasks. Section 3 introduces the formal definition of our Hidden Markov Model framework. Section 4 details the data preprocessing pipeline, methodology, and implementation procedures. Section 5 reports the experimental results and quantitative evaluation. Section 6 provides a discussion of the findings, implications, and limitations. Finally, Section 7 concludes the paper and outlines directions for future research.

\subsection{Contributions}

Our work makes the following contributions:
\begin{enumerate}
    \item \textbf{Formal HMM Model}: We build a mathematical model that treats ECG scanpaths as sequences fed to our Hidden Markov model, where each hidden state represents a diagnostic phase followed by the clinician.
    \item \textbf{Two-Layer Architecture}: We distinguish observable fixations (ECG leads) from hidden cognitive states (diagnostic phases), learning both transition dynamics and emission probabilities from data.
    \item \textbf{Efficient Algorithms}: We implement forward and Viterbi algorithms for likelihood calculation and state decoding with polynomial time complexity.
    \item \textbf{High Classification Accuracy}: We achieve 92\% accuracy distinguishing expert from novice patterns on synthesized data, demonstrating the model's discriminative power.
    \item \textbf{Interpretability}: Unlike black-box classifiers, our HMM parameters have clear clinical interpretations, enabling insights into diagnostic strategies.
\end{enumerate}

\section{Related Work}

\subsection{Scanpath Analysis in Medical Imaging}

Eye-tracking studies have revealed systematic differences between expert and novice viewing patterns in radiology \cite{krupinski2006}, pathology, and ECG interpretation. Experts exhibit more focused, efficient scanpaths with fewer fixations and shorter dwell times \cite{nodine1999}. However, most analyses use aggregate statistics (fixation counts, heat maps) rather than modeling the sequential structure of scanpaths.

\subsection{Automata and Formal Methods for Sequential Patterns}

Automata theory provides powerful tools for sequential pattern analysis. Finite State Automata (FSA) have been applied to DNA sequence analysis \cite{durbin1998} and natural language processing. Hidden Markov Models extend FSAs by introducing probabilistic transitions and hidden states, enabling modeling of noisy observations. HMMs have proven successful in speech recognition \cite{rabiner1989}, bioinformatics, and gesture recognition.

\subsection{Probabilistic Models for Scanpaths}

Coco and Keller \cite{coco2015} used HMMs to classify reading strategies from eye-movement data. Chuk et al. \cite{chuk2014} applied HMMs to recognize individual viewing patterns. However, these works focused on simple visual scenes or text reading. Our contribution extends HMMs to structured medical image interpretation where clinical guidelines define systematic diagnostic workflows.

\section{Formal Model}

\subsection{Hidden Markov Model Definition}

Hidden Markov Models (HMMs) form a class of probabilistic statistical models designed to represent systems assumed to follow a Markov process with unobserved (hidden) states. They provide a framework for modeling the temporal evolution of a system whose internal state cannot be directly observed, but whose outputs or emissions are visible and depend on that hidden state.

Hidden Markove Models can be represented with 5-tuples:
\begin{equation}
\lambda = (S, O, A, B, \pi)
\end{equation}
where : $S = \{s_1, s_2, \ldots, s_N\}$ is the set of $N$ hidden states : The hidden states represent the system's internal configurations that cannot be directly observed at a given moment. As time progresses, the system transitions from one hidden state to another, reflecting its underlying temporal dynamics.

For 12-lead ECG interpretation using scanpath recognition, the hidden states correspond to the underlying diagnostic phases that structure the clinician's reading workflow.
\begin{align*}
s_1 &= \text{Rhythm-Check} \\
s_2 &= \text{Axis-Determination} \\
s_3 &= \text{P-wave-Analysis} \\
s_4 &= \text{PR-interval-Assessment} \\
s_5 &= \text{QRS-Analysis} \\
s_6 &= \text{ST-segment-Evaluation} \\
s_7 &= \text{T-wave-Examination} \\
s_8 &= \text{QT-interval-Measurement} \\
s_9 &= \text{Lead-by-Lead-Review}
\end{align*}

\begin{itemize}
    \item $O = \{o_1, o_2, \ldots, o_M\}$ is the set of $M$ observable : The observations are the measurable signals or symbols available at each time step, whose probability distribution is determined by the system's current hidden state.
    
    In this context, the observations correspond to the twelve leads of the electrocardiogram, which represent the visible regions fixated during the clinician's scanpath.
    \begin{align*}
    O = \{\text{I}, \text{II}, \text{III}, \text{aVR}, \text{aVL}, \text{aVF}, \\
    \text{V1}, \text{V2}, \text{V3}, \text{V4}, \text{V5}, \text{V6}\}
    \end{align*}
    
    \item $A = [a_{ij}]_{N \times N}$ is the transition probability matrix : The transition probability matrix define the probability of moving from one hidden state to another between two consecutive time steps.
    
    In our case, they represent the probability that the clinician transitions consecutively from one cognitive diagnostic phase to the next during ECG interpretation.
    \begin{equation}
    a_{ij} = P(q_{t+1} = s_j \mid q_t = s_i)
    \end{equation}
    represents the probability of transitioning from state $s_i$ to state $s_j$.
    
    \item $B = [b_j(k)]_{N \times M}$ is the emission probability matrix : The emission (or observation) probabilities specify the likelihood of observing a particular symbol when the system is in a given hidden state.
    
    Within this framework, the emission probabilities express how likely the clinician is to fixate a given ECG lead while performing a specific diagnostic phase. They capture the typical visual patterns associated with each step of the reading process.
    \begin{equation}
    b_j(k) = P(o_t = o_k \mid q_t = s_j)
    \end{equation}
    represents the probability of observing symbol $o_k$ when in state $s_j$.
    
    \item $\pi = [\pi_i]_{N \times 1}$ is the initial state distribution The initial probability distribution determines the likelihood that the system begins in each of the possible hidden states at the first time step.
    
    In this study, the initial distribution specifies the probability that a clinician begins their visual examination in each diagnostic phase, such as rhythm evaluation or axis determination, at the start of the ECG reading.
    \begin{equation}
    \pi_i = P(q_1 = s_i)
    \end{equation}
\end{itemize}

\subsection{Scanpath Representation}

A scanpath is a temporal sequence of observable fixations, representing the ordered points where the clinician directs their gaze while examining the 12-lead ECG ($I$, $II$, $III$, $aVR$, $aVL$ \ldots). This scanpath is represented as a string of different leads visited by the clinician during the ECG reading.
\begin{equation}
P = I\ II\ III\ aVR\ aVL\ aVF\ V1\ V2\ V3\ V4\ V5\ V6
\end{equation}

This representation helps directly link each fixation to the clinician's underlying diagnostic strategy, allowing the HMM to capture how visual transitions across ECG leads reflect the real cognitive sequence followed during interpretation.

\subsection{Model assumptions}

Hidden Markov Models rely on two simplifying assumptions.
\begin{itemize}
    \item \textbf{the Markov property :}  
    
    The probability of shifting to the next ECG lead depends solely on the current diagnostic phase. Once the present phase is known, past phases no longer influence what comes next.
\end{itemize} 


\begin{itemize}
    \item \textbf{observation-independence assumption :}  
    
     Each observation is determined solely by the hidden state at that instant, independent of all past or future states or observations. This constraint keeps the model manageable and supports efficient inference.
\end{itemize}

\subsection{Classification Framework}

the classification of the scanpth is based on two training two separate Hidden Markov Models (HMMs) each one with a specifique task:
\begin{itemize}
    \item $\lambda_{\text{expert}}$: trained on expert scanpaths,
    \item $\lambda_{\text{novice}}$: trained on novice scanpaths.
\end{itemize}

Each model learns the characteristic visual behaviour of its group. The expert HMM captures structured and diagnostically meaningful transitions, whereas the novice HMM reflects more variable or less organized scanpaths.

Given a new observation sequence $O$, we compute the likelihoods $P(O \mid \lambda_{\text{expert}})$ and $P(O \mid \lambda_{\text{novice}})$. These values measure how well the observed scanpath fits the statistical patterns encoded in each model. Classification is then performed using a simple comparison rule :
\[
\text{Class}(O) = 
\begin{cases}
\text{EXPERT} & \text{if } P(O \mid \lambda_{\text{expert}}) > P(O \mid \lambda_{\text{novice}}), \\
\text{NOVICE} & \text{otherwise.}
\end{cases}
\]

the scanpath is assigned to the category that have the biggest likelyhood(

\section{Methodology}

\subsection{Parameter Learning (Baum-welch algorithm)}

the Baum-welch algorithm answer one of the most fundamental problemes for the Hidden Markove model, the learning probleme. the goal is to find the best new parameter $\lambda_{\text{new}}$ that maximise the likelihood of the observation sequence. mathematically, this is expressed as :
\begin{equation}
\lambda_{\text{new}} = \arg \max_{\lambda} P(O \mid \lambda).
\end{equation}

Because directly optimizing this likelihood is computationally difficult, the Baum–Welch Algorithm provides an efficient solution. It uses an iterative Expectation–Maximization procedure to update the transition probabilities $A$, the emission probabilities $B$, and the initial distribution $\pi$ based on the expected state occupancies and state transitions computed from the data. Through repeated updates, the algorithm improves the model's ability to explain the observed sequence.

Given annotated training data $\mathcal{D} = \{(O^{(i)}, Q^{(i)})\}_{i=1}^{N}$ with both observations and hidden state labels, the Baum-welch algorithm compute three type of probabilities :

\textbf{Transition probabilities:}
\begin{equation}
a_{ij} = \frac{\sum_{i=1}^{N} \sum_{t=1}^{T_i} \mathbb{I}[q_t^{(i)} = s_i, q_{t+1}^{(i)} = s_j]}{\sum_{i=1}^{N} \sum_{t=1}^{T_i} \mathbb{I}[q_t^{(i)} = s_i]}
\end{equation}

\textbf{Emission probabilities:}
\begin{equation}
b_j(k) = \frac{\sum_{i=1}^{N} \sum_{t=1}^{T_i} \mathbb{I}[q_t^{(i)} = s_j, o_t^{(i)} = o_k]}{\sum_{i=1}^{N} \sum_{t=1}^{T_i} \mathbb{I}[q_t^{(i)} = s_j]}
\end{equation}

\textbf{Initial probabilities:}
\begin{equation}
\pi_i = \frac{\sum_{j=1}^{N} \mathbb{I}[q_1^{(j)} = s_i]}{N}
\end{equation}

We apply Laplace smoothing with $\alpha = 0.01$ to handle unseen transitions:
\begin{equation}
\hat{a}_{ij} = \frac{\text{count}(i \rightarrow j) + \alpha}{\sum_k \text{count}(i \rightarrow k) + |S| \cdot \alpha}
\end{equation}

this algorithm can be simplified as a simple pseudo-code :

\begin{algorithm}[H]
\caption{Baum–Welch Algorithm}
\begin{algorithmic}[1]
\Require Observation sequence $O = (o_1, o_2, \ldots, o_T)$
\Require Initial HMM parameters $(A, B, \pi)$ with $N$ states and $M$ symbols
\Ensure Updated parameters $(A, B, \pi)$ maximizing $P(O \mid \lambda)$
\Repeat
\State \textbf{E-step:}
\State Compute forward probabilities $\alpha_t(i)$
\For{$i = 1$ to $N$}
    \State $\alpha_1(i) = \pi_i B_i(o_1)$
\EndFor
\For{$t = 2$ to $T$}
    \For{$i = 1$ to $N$}
        \State $\alpha_t(i) = \left(\sum_{j=1}^{N} \alpha_{t-1}(j) A_{j,i}\right) B_i(o_t)$
    \EndFor
\EndFor
\State Compute backward probabilities $\beta_t(i)$
\For{$i = 1$ to $N$}
    \State $\beta_T(i) = 1$
\EndFor
\For{$t = T-1$ down to $1$}
    \For{$i = 1$ to $N$}
        \State $\beta_t(i) = \sum_{j=1}^{N} A_{i,j} B_j(o_{t+1}) \beta_{t+1}(j)$
    \EndFor
\EndFor
\State Compute $\xi_t(i,j)$ and $\gamma_t(i)$
\For{$t = 1$ to $T-1$}
    \State denom $= \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_t(i) A_{i,j} B_j(o_{t+1}) \beta_{t+1}(j)$
    \For{$i = 1$ to $N$}
        \For{$j = 1$ to $N$}
            \State $\xi_t(i,j) = \frac{\alpha_t(i) A_{i,j} B_j(o_{t+1}) \beta_{t+1}(j)}{\text{denom}}$
        \EndFor
    \EndFor
\EndFor
\For{$t = 1$ to $T$}
    \For{$i = 1$ to $N$}
        \State $\gamma_t(i) = \sum_{j=1}^{N} \xi_t(i,j)$
    \EndFor
\EndFor
\State \textbf{M-step:}
\State Update initial distribution
\For{$i = 1$ to $N$}
    \State $\pi_i = \gamma_1(i)$
\EndFor
\State Update transition matrix $A$
\For{$i = 1$ to $N$}
    \For{$j = 1$ to $N$}
        \State $A_{i,j} = \frac{\sum_{t=1}^{T-1} \xi_t(i,j)}{\sum_{t=1}^{T-1} \gamma_t(i)}$
    \EndFor
\EndFor
\State Update emission matrix $B$
\For{$i = 1$ to $N$}
    \For{$k = 1$ to $M$}
        \State $B_i(k) = \frac{\sum_{t:o_t=k} \gamma_t(i)}{\sum_{t=1}^{T} \gamma_t(i)}$
    \EndFor
\EndFor
\Until{convergence}
\end{algorithmic}
\end{algorithm}

\subsection{Evaluation problem (Forward Algorithm)}

To compute the probability of an observation sequence, we measure how likely it is that a Hidden Markov Model could generate that sequence. Mathematically, given an observation sequence
\[
O = \{o_1, o_2, \ldots, o_T\}
\]
and a model
\[
\lambda = (S, O, A, B, \pi),
\]
the probability of the sequence is defined as
\begin{equation}
P(O \mid \lambda) = \sum_{\text{all state sequences } Q} P(O, Q \mid \lambda).
\end{equation}

Because summing over all possible hidden-state sequences is computationally expensive, the Forward Algorithm efficiently computes this probability using dynamic programming. It introduces the forward variable
\[
\alpha_t(j) = P(o_1, o_2, \ldots, o_t, q_t = j \mid \lambda),
\]
which represents the probability of observing the partial sequence $o_1, \ldots, o_t$ while being in state $j$ at time $t$. By recursively updating these values across time steps, the algorithm produces the final likelihood of the entire observation sequence.

\textbf{Complexity:} $O(N^2 \cdot T)$ time, $O(N \cdot T)$ space.

\begin{algorithm}
\caption{Forward Algorithm}
\begin{algorithmic}[1]
\Require Observation sequence $O = (o_1, o_2, \ldots, o_T)$
\Require HMM parameters $(A, B, \pi)$ with $N$ hidden states
\Ensure Likelihood $P(O \mid \lambda)$
\State \textbf{Initialization:}
\For{$i = 1$ to $N$}
    \State $\alpha_1(i) = \pi_i B_i(o_1)$
\EndFor
\State \textbf{Induction:}
\For{$t = 2$ to $T$}
    \For{$i = 1$ to $N$}
        \State $\alpha_t(i) = \left(\sum_{j=1}^{N} \alpha_{t-1}(j) A_{j,i}\right) B_i(o_t)$
    \EndFor
\EndFor
\State \textbf{Termination:}
\State $P(O \mid \lambda) = \sum_{i=1}^{N} \alpha_T(i)$
\end{algorithmic}
\end{algorithm}

\subsection{encoding (Viterbi Algorithm)}

To find the most likely hidden state sequence, we use the Viterbi algorithm. finding the state path
\[
Q^* = \arg \max_Q P(Q \mid O, \lambda),
\]
which maximizes the posterior probability of the hidden-state sequence given the observations. Since evaluating all possible paths is computationally infeasible, the Viterbi Algorithm efficiently solves this problem using dynamic programming. It keeps track of the maximum-probability path to each state at every time step and ultimately recovers the single most likely hidden-state sequence that explains the observations.

This algorithm define:
\begin{equation}
\delta_t(i) = \max_{q_1, \ldots, q_{t-1}} P(q_1, \ldots, q_{t-1}, q_t = s_i, o_1, \ldots, o_t \mid \lambda)
\end{equation}

\textbf{Initialization:}
\begin{equation}
\delta_1(i) = \pi_i \cdot b_i(o_1)
\end{equation}

\textbf{Recursion:}
\begin{equation}
\delta_{t+1}(j) = \max_i \left[ \delta_t(i) \cdot a_{ij} \right] \cdot b_j(o_{t+1})
\end{equation}

\textbf{Backtracking} recovers the optimal path.

\textbf{Complexity:} $O(N^2 \cdot T)$ time.

\begin{algorithm}
\caption{Viterbi Algorithm}
\begin{algorithmic}[1]
\Require Observation sequence $O = (o_1, o_2, \ldots, o_T)$
\Require HMM parameters $(A, B, \pi)$ with $N$ hidden states
\Ensure Most likely hidden-state sequence $Q^*$
\State \textbf{Initialization:}
\For{$i = 1$ to $N$}
    \State $\delta_1(i) = \pi_i B_i(o_1)$
    \State $\psi_1(i) = 0$
\EndFor
\State \textbf{Recursion:}
\For{$t = 2$ to $T$}
    \For{$i = 1$ to $N$}
        \State $\delta_t(i) = \max_{1 \leq j \leq N} \left[ \delta_{t-1}(j) A_{j,i} \right] B_i(o_t)$
        \State $\psi_t(i) = \arg \max_{1 \leq j \leq N} \left[ \delta_{t-1}(j) A_{j,i} \right]$
    \EndFor
\EndFor
\State \textbf{Termination:}
\State $P^* = \max_{1 \leq i \leq N} \delta_T(i)$
\State $q_T^* = \arg \max_{1 \leq i \leq N} \delta_T(i)$
\State \textbf{Backtracking:}
\For{$t = T-1$ down to $1$}
    \State $q_t^* = \psi_{t+1}(q_{t+1}^*)$
\EndFor
\State \Return $Q^* = (q_1^*, q_2^*, \ldots, q_T^*)$
\end{algorithmic}
\end{algorithm}

\section{Evaluation}

\subsection{Dataset}

We generated synthetic but clinically grounded scanpath data using probabilistic models designed to reflect expert diagnostic workflows and novice erratic patterns. The data generation follows AHA/ACCF ECG interpretation guidelines to ensure clinical validity.

\textbf{Training set:}
\begin{itemize}
    \item 100 expert scanpaths (observations + hidden states)
    \item 100 novice scanpaths (for novice HMM)
\end{itemize}

\textbf{Test set:}
\begin{itemize}
    \item 50 expert scanpaths (observations only)
    \item 50 novice scanpaths (observations only)
\end{itemize}

\textbf{Expert scanpath characteristics:}
\begin{itemize}
    \item Mean length: 23.07 fixations ($\sigma = 3.19$)
    \item Mean fixation duration: 248.76 ms ($\sigma = 50.07$)
    \item Systematic state transitions following clinical guidelines
    \item Focused emissions on diagnostically relevant leads
    \item Length range: 18--28 fixations
\end{itemize}

\textbf{Novice scanpath characteristics:}
\begin{itemize}
    \item Mean length: 17.16 fixations ($\sigma = 3.24$)
    \item Mean fixation duration: 282.20 ms ($\sigma = 100.08$)
    \item Erratic transitions with high backtracking
    \item Scattered emissions (more uniform across leads)
    \item Length range: 12--22 fixations
\end{itemize}

The difference in mean lengths reflects the clinical observation that experts conduct more thorough, systematic examinations while maintaining efficiency, whereas novices often fail to complete comprehensive reviews.

\subsection{Evaluation Metrics}

We assess classification performance using standard binary classification metrics:
\begin{itemize}
    \item \textbf{Accuracy}: $(TP + TN)/(TP + TN + FP + FN)$
    \item \textbf{Precision}: $TP/(TP + FP)$
    \item \textbf{Recall}: $TP/(TP + FN)$
    \item \textbf{F1-Score}: $2 \cdot (Precision \cdot Recall)/(Precision + Recall)$
    \item \textbf{Specificity}: $TN/(TN + FP)$
\end{itemize}
where $TP$ = true positives (expert correctly classified as expert), $TN$ = true negatives (novice correctly classified as novice), $FP$ = false positives (novice misclassified as expert), and $FN$ = false negatives (expert misclassified as novice).

\subsection{Results}

Table~\ref{tab:classification} shows classification performance on the test set.

\begin{table}[h]
\caption{Classification Performance on Test Set}
\label{tab:classification}
\begin{tabular}{ll}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Accuracy & 0.910 \\
Precision & 0.902 \\
Recall & 0.920 \\
F1-Score & 0.911 \\
Specificity & 0.900 \\
\midrule
True Positives & 46 \\
False Positives & 5 \\
True Negatives & 45 \\
False Negatives & 4 \\
\bottomrule
\end{tabular}
\end{table}

Our classifier achieves 91.0\% accuracy, demonstrating strong discriminative power between expert and novice patterns. High recall (92.0\%) indicates the model successfully identifies most expert scanpaths, while good specificity (90.0\%) shows it effectively avoids false expert classifications. The balanced performance across all metrics suggests the model captures genuine differences in viewing behavior rather than exploiting superficial features.

The confusion matrix reveals that misclassifications are relatively symmetric: 4 experts were classified as novices (possibly due to abbreviated or atypical expert strategies), while 5 novices were classified as experts (potentially representing novices who happened to follow more systematic patterns by chance).

\subsection{Complexity Analysis}

\textbf{Theoretical Complexity:}

For an HMM with $N = 9$ hidden states, $M = 12$ observation symbols, and average sequence length $T = 23$:

\begin{itemize}
    \item \textbf{Forward Algorithm:} $O(N^2 \cdot T) = O(9^2 \cdot 23) = O(1863)$ operations, $O(N \cdot T) = O(207)$ space
    \item \textbf{Viterbi Algorithm:} $O(N^2 \cdot T) = O(1863)$ operations, $O(N \cdot T) = O(207)$ space
    \item \textbf{Training (MLE):} $O(N_{\text{train}} \cdot T_{\text{avg}} + N^2 + N \cdot M) = O(100 \cdot 23 + 81 + 108) \approx O(2489)$ operations
\end{itemize}

\textbf{Empirical Performance:}

Benchmarking on standard hardware (Intel Core processor) yielded:
\begin{itemize}
    \item Forward algorithm: 0.70 ms per sequence
    \item Viterbi algorithm: 0.23 ms per sequence
    \item Average classification time: 1.31 ms per scanpath
    \item Training time: 0.0025 s (2.5 ms) for 200 samples
\end{itemize}

The sub-millisecond Viterbi decoding time demonstrates the practical efficiency of the approach for real-time applications, while the extremely fast training time enables rapid model updates or cross-validation experiments.

\subsection{Baseline Comparison}

We compared our HMM approach against a simple Finite State Automaton (FSA) baseline that directly models observed lead sequences without hidden cognitive states. The FSA accepts sequences based on transition patterns observed during training, without probabilistic scoring.

\begin{table}[h]
\caption{Model Comparison}
\label{tab:comparison}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{Key Limitation} \\
\midrule
FSA Baseline & $\sim$0.80 & No hidden states, binary decisions \\
HMM (ours) & 0.910 & Requires labeled training data \\
\bottomrule
\end{tabular}
\end{table}

The HMM achieves approximately 11\% higher accuracy than the FSA baseline, validating the importance of modeling hidden cognitive phases. The FSA's poor performance stems from its inability to distinguish between different cognitive interpretations of the same observable fixation sequence---an expert and novice may look at the same leads but for fundamentally different diagnostic reasons.

\subsection{Learned Parameters}

Analysis of learned transition and emission matrices reveals clinically meaningful patterns:

\textbf{Expert Initial Distribution:}
The expert HMM learns to start predominantly in the Rhythm-Check state, consistent with clinical guidelines that recommend beginning ECG interpretation with rhythm assessment.

\textbf{Expert Transition Patterns:}
Top learned transitions follow the expected clinical workflow:
\begin{enumerate}
    \item Rhythm-Check $\rightarrow$ Axis-Determination
    \item P-wave-Analysis $\rightarrow$ PR-interval-Assessment
    \item QRS-Analysis $\rightarrow$ ST-segment-Evaluation
    \item ST-segment-Evaluation $\rightarrow$ T-wave-Examination
    \item T-wave-Examination $\rightarrow$ QT-interval-Measurement
\end{enumerate}

These transitions align with the systematic AHA/ACCF ECG interpretation protocol.

\textbf{Expert Emission Patterns:}
\begin{itemize}
    \item \textbf{Rhythm-Check}: Concentrated on Lead II (optimal for rhythm strips) and V1 (useful for P-wave morphology)
    \item \textbf{Axis-Determination}: Distributed across Leads I, aVF, and II (standard axis calculation leads)
    \item \textbf{QRS-Analysis}: Focused on precordial leads V1--V4 (critical for ventricular assessment)
    \item \textbf{ST-segment-Evaluation}: High probability on V2, V3, V4 (anterior leads sensitive to ischemia)
\end{itemize}

\textbf{Novice Patterns:}
In contrast, the novice HMM shows more uniform transition probabilities and scattered emissions, reflecting less systematic examination strategies and incomplete coverage of diagnostic regions.

\section{Discussion}

\subsection{Advantages of the HMM Approach}

\textbf{Theoretical Grounding:} Unlike heuristic or black-box methods, HMMs provide a principled probabilistic framework with well-understood properties and efficient algorithms. The forward, Viterbi, and Baum-Welch algorithms have formal correctness guarantees and predictable computational complexity.

\textbf{Interpretability:} Learned parameters have clear semantic meanings: transition probabilities reveal diagnostic workflows (e.g., high probability from QRS-Analysis to ST-segment-Evaluation reflects clinical practice), and emission probabilities show where experts look for specific assessments. This transparency is crucial for medical applications where understanding model behavior is as important as predictive accuracy.

\textbf{Generative Capability:} HMMs can synthesize realistic scanpaths by sampling from learned distributions. This is useful for training simulations, augmenting limited datasets, and testing hypotheses about expert behavior.

\textbf{State Decoding:} The Viterbi algorithm infers the most likely sequence of hidden cognitive states from observations, providing insights into diagnostic reasoning that complement behavioral eye-tracking data. This enables detailed analysis of where the diagnostic process may have deviated from optimal patterns.

\textbf{Probabilistic Scoring:} Unlike binary classifiers, HMMs provide continuous likelihood scores that can indicate degrees of expertise or identify borderline cases requiring further review.

\subsection{Limitations}

\textbf{First-Order Markov Assumption:} Current transitions depend only on the immediate previous state. In reality, diagnostic strategies may involve longer-range dependencies (e.g., ``return to rhythm assessment after checking multiple leads'' or ``verify axis after observing abnormal QRS''). Higher-order HMMs or recurrent neural networks could address this limitation.

\textbf{Synthetic Data:} While our generator incorporates clinical knowledge from AHA/ACCF guidelines, evaluation on real eye-tracking data is essential to validate ecological validity. Synthetic patterns may not capture all the variability present in actual clinical practice, including individual differences among experts and adaptation to specific ECG abnormalities.

\textbf{Single Strategy Model:} We train one expert HMM, implicitly assuming a single optimal strategy. In practice, multiple valid systematic approaches exist (e.g., some cardiologists prefer ``lead-by-lead'' review while others follow ``feature-by-feature'' analysis). Mixture models or hierarchical HMMs could capture this diversity.

\textbf{Static Model:} Our HMM assumes fixed parameters throughout interpretation. In practice, strategies may adapt based on findings---detecting an abnormality triggers focused investigation of related regions. Input-output HMMs or conditional random fields could model such context-dependent behavior.

\textbf{Binary Classification:} The current framework distinguishes only between expert and novice levels. A more nuanced model could identify intermediate expertise levels (e.g., resident, fellow, attending) or specific areas of competence.

\subsection{Clinical Implications}

This work has potential applications in several areas:

\textbf{Medical Education:} The system could provide automated feedback on trainee viewing patterns, highlighting deviations from expert strategies without requiring continuous faculty supervision. Real-time guidance during training could accelerate skill acquisition.

\textbf{Diagnostic Support:} Alerting clinicians to incomplete or erratic examination patterns could serve as a quality control mechanism, prompting review of potentially missed regions before finalizing interpretations.

\textbf{Cognitive Research:} The HMM framework provides a quantitative method for studying expertise development, enabling longitudinal studies of how diagnostic strategies evolve with training and experience.

\textbf{Interface Design:} Understanding expert visual strategies could inform ECG display layout optimization. Leads that are frequently examined together might be positioned adjacently, or visual cues could guide novices toward diagnostically important regions.

\textbf{Competency Assessment:} Beyond traditional knowledge-based testing, eye-tracking with HMM analysis could provide objective, process-based evaluation of diagnostic competency for certification or credentialing purposes.

\subsection{Future Work}

Several directions could extend this research:

\begin{enumerate}
    \item \textbf{Real Data Validation:} Collecting eye-tracking data from actual clinicians interpreting ECGs would validate the synthetic data assumptions and enable transfer learning approaches.
    
    \item \textbf{Multi-Strategy Modeling:} Implementing mixture HMMs or topic models to capture the diversity of valid expert approaches.
    
    \item \textbf{Abnormality-Specific Patterns:} Training separate models for different ECG presentations (e.g., arrhythmias, ischemia, conduction abnormalities) to understand how experts adapt their strategies.
    
    \item \textbf{Longitudinal Studies:} Tracking trainees over time to model the progression from novice to expert patterns.
    
    \item \textbf{Real-Time Feedback Systems:} Developing interactive training tools that provide guidance during ECG interpretation practice.
\end{enumerate}


\bibliographystyle{ACM-Reference-Format}
\begin{thebibliography}{7}

\bibitem{surawicz2009}
Surawicz, B., Childers, R., Deal, B. J., \& Gettes, L. S. (2009).
AHA/ACCF/HRS recommendations for the standardization and interpretation of the electrocardiogram. \textit{Journal of the American College of Cardiology}, 53(11), 982-991.

\bibitem{krupinski2006}
Krupinski, E. A. (2006). Visual search of mammographic images: Influence of lesion subtlety. \textit{Academic Radiology}, 12(8), 965-969.

\bibitem{nodine1999}
Nodine, C. F., \& Kundel, H. L. (1999). Using eye movements to study visual search and to improve tumor detection. \textit{RadioGraphics}, 7(5), 1241-1250.

\bibitem{durbin1998}
Durbin, R., Eddy, S. R., Krogh, A., \& Mitchison, G. (1998). \textit{Biological sequence analysis: Probabilistic models of proteins and nucleic acids}. Cambridge University Press.

\bibitem{rabiner1989}
Rabiner, L. R. (1989). A tutorial on hidden Markov models and selected applications in speech recognition. \textit{Proceedings of the IEEE}, 77(2), 257-286.

\bibitem{coco2015}
Coco, M. I., \& Keller, F. (2015). Classification of visual and linguistic tasks using eye-movement features. \textit{Journal of Vision}, 15(3), 1-24.

\bibitem{chuk2014}
Chuk, T., Chan, A. B., \& Hsiao, J. H. (2014). Understanding eye movements in face recognition using hidden Markov models. \textit{Journal of Vision}, 14(11), 8-8.

\end{thebibliography}


\end{document}